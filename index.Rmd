---
title       : Introduction of Feature Hashing
subtitle    : Create a Model Matrix via Feature Hashing with a Formula Interface
author      : Wush Wu
job         : Taiwan R User Group
framework   : io2012-wush
highlighter : highlight.js
hitheme     : zenburn
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
knit        : slidify::knit2slides
--- &vcenter .largecontent

```{r setup, include=FALSE, cache=FALSE}
suppressPackageStartupMessages({
  library(knitr)
  library(ggplot2)
  library(xtable)
  library(data.table)
  library(magrittr)
  library(dplyr)
  library(xgboost)
  library(FeatureHashing)
})
opts_chunk$set(echo = FALSE, cache = FALSE)
fig <- function(path, size = 100, style = NULL) {
  if (is.null(style)) sprintf('<img src="assets/img/%s" style="max-height: %d%%; max-width: %d%%;"/>', path, size, size)
  else sprintf('<img src="assets/img/%s" style="%s"/>', path, style)
}
center_fig <- function(path, size = 100, style = NULL) {
  sprintf('<center>%s</center>', fig(path, size, style))
}
```

## [Wush Chi-Hsuan Wu](https://github.com/wush978)

- Ph.D. Student
    - Display Advertising
    - Large Scale Machine Learning
- Familiar tools: R, C++, [Apache Spark](http://spark.apache.org/)

--- &twocol

## Taiwan, a Mountainous Island

*** =left

`r center_fig("TaiwanMap.jpg", 80)`

<small>source: <http://www.nbcnews.com> </small>

*** =right

- Highest Mountain: Yushan (3952m)

`r center_fig("Mount_Yu_Shan_-_Taiwan.jpg")`

--- &twocol

## Taipei City and Taipei 101

`r center_fig("tpnightview.jpg", 75)`
<small>source: <https://c2.staticflickr.com/6/5208/5231215951_c0e0036b17_b.jpg></small>

---

## New Landmark of Taipei

`r center_fig("mailbox.jpg")`

--- &vcenter .largecontent

## Taiwan R User Group

<http://www.meetup.com/Taiwan-R>

`r center_fig("taiwan-r.jpg", 60)`

--- .dark .segue

## An Example of FeatureHashing

--- &vcenter .largecontent

## Sentiment Analysis

- Provided by [Lewis Crouch](https://github.com/lewis-c)
- Show the pros of using `FeatureHashing`

```{r imdb.setup, include=FALSE}
opts_chunk$set(cache = TRUE)
```

--- &vcenter .largecontent

## Dataset: [IMDB](https://www.kaggle.com/c/word2vec-nlp-tutorial)

- Predict the rating of the movie according to the text in the review.

```{r imdb}
imdb <- read.delim("labeledTrainData.tsv", quote = "", as.is = TRUE)
imdb$review <- tolower(gsub("[^[:alnum:] ]", " ", imdb$review))
```

- Training Dataset: 25000 reviews
- Binary response: positive or negative.

```{r imdb.sentiment, dependson="imdb"}
print(imdb$sentiment[457])
```

- Cleaned review

```{r imdb.review, dependson="imdb"}
strwrap(imdb$review[457], width = 80)
```

```{r imdb.split, dependson="imdb"}
imdb.shuffle <- dplyr::sample_frac(imdb, 1, replace = FALSE)
imdb.train <- head(imdb.shuffle, 20000)
imdb.valid <- tail(imdb.shuffle, 5000)
```

--- &vcenter .largecontent

## Word Segmentation

```{r imdb.tokenize, dependson="imdb"}
strsplit(imdb$review[457], " ")[[1]]
```


--- &vcenter .largecontent

## Word Segmentation and Feature Extraction

```{r imdb.tokenize.vectorize, dependson="imdb"}
tokens <- strsplit(head(imdb$review), " ")
tks <- c("message", "hate", "bad", "like", "gratuitous")
tmp <- lapply(tks, function(tk) {
  sapply(tokens, `%in%`, x = tk)  
}) %>% 
  `names<-`(tks) %>% 
  as.data.frame(stringsAsFactors = FALSE)
tmp$id <- imdb$id %>% head
kable(tmp[,c(6,1:5)])
```

--- &vcenter .largecontent

## `FeatureHashing`

- `FeatureHashing` implements `split` in the formula interface.

```{r imdb.m, dependson="imdb.split", echo = TRUE}
hash_size <- 2^16
m.train <- hashed.model.matrix(~ split(review, delim = " ", type = "existence"), 
                               data = imdb.train, 
                               hash.size = hash_size,
                               signed.hash = FALSE)
m.valid <- hashed.model.matrix(~ split(review, delim = " ", type = "existence"), 
                              data = imdb.valid, 
                              hash.size = hash_size,
                              signed.hash = FALSE)
```

--- &vcenter .largecontent

## Type of `split`

- `existence`
- `count`
- [`tf-idf`](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) which is contributed by [MichaÃ«l Benesty](https://github.com/pommedeterresautee) and will be announced in v0.9.1

--- &vcenter .largecontent

## Gradient Boosted Decision Tree with `xgboost`

```{r, dependson="imdb.m", echo = TRUE, results='hide'}
dtrain <- xgb.DMatrix(m.train, label = imdb.train$sentiment)
dvalid <- xgb.DMatrix(m.valid, label = imdb.valid$sentiment)
watch <- list(train = dtrain, valid = dvalid)
g <- xgb.train(booster = "gblinear", nrounds = 100, eta = 0.0001, max.depth = 2,
                data = dtrain, objective = "binary:logistic",
                watchlist = watch, eval_metric = "auc")
```

```
[0]  train-auc:0.969895  valid-auc:0.914488
[1]	train-auc:0.969982	valid-auc:0.914621
[2]	train-auc:0.970069	valid-auc:0.914766
...
[97]  train-auc:0.975616	valid-auc:0.922895
[98]	train-auc:0.975658	valid-auc:0.922952
[99]	train-auc:0.975700	valid-auc:0.923014
```

--- &vcenter .largecontent

## Performance

- The [AUC](https://www.kaggle.com/wiki/AUC) of `g` is $0.90110$ in the [public leader board](https://www.kaggle.com/c/word2vec-nlp-tutorial/leaderboard?submissionId=1848642) 
    - It outperforms the benchmark in Kaggle

--- &vcenter .largecontent

## The Purpose of FeatureHashing

- **Make our life easier**

--- .segue .dark

```{r, include=FALSE}
opts_chunk$set(cache = FALSE)
```

## Formula Interface in R

--- &vcenter .largecontent

## Algorithm in Text Book

### Regression

$$y = X \beta + \varepsilon$$

### Real Data

```{r iris}
iris.demo <- iris[c(1:2,51:52,101:102),]
kable(iris.demo)
```

- How to convert the real data to $X$?

--- &vcenter .largecontent

## Feature Vectorization and Formula Interface

- $X$ is usually constructed via `model.matrix` in R

```{r model.matrix, echo = TRUE, results='hide'}
model.matrix(~ ., iris.demo)
```

```{r model.matrix.result}
kable(model.matrix(~ ., iris.demo))
```

--- &vcenter .largecontent

## Formula Interface

- `y ~ a + b`
    - `y` is the response
    - `a` and `b` are predictors

```{r data.demo, echo = FALSE, eval = TRUE, results='hide'}
data.demo <- iris.demo
colnames(data.demo) <- c("a", "b", "c", "d", "e")
```

--- &vcenter .largecontent

## Formula Interface: `+`

- `+` is the operator of combining linear predictors

```{r fi.+1, echo = TRUE, eval = FALSE}
model.matrix(~ a + b, data.demo)
```

```{r fi.+2}
kable(model.matrix(~ a + b, data.demo))
```

--- &vcenter .largecontent

## Formula Interface: `:`

- `:` is the interaction operator

```{r fi.:1, echo = TRUE, eval = FALSE}
model.matrix(~ a + b + a:b, data.demo)
```

```{r fi.:2}
kable(model.matrix(~ a + b + a:b, data.demo))
```

--- &vcenter .largecontent

## Formula Interface: `*`

- `*` is the operator of cross product

```{r fi.*1, echo = TRUE, eval = FALSE}
# a + b + a:b
model.matrix(~ a * b, data.demo)
```

```{r fi.*2}
kable(model.matrix(~ a * b, data.demo))
```

--- &vcenter .largecontent

## Formula Interface: `(`

- `:` and `*` are distributive over `+`

```{r fi.(1, echo = TRUE, eval = FALSE}
# a:c + b:c
model.matrix(~ (a + b):c, data.demo)
```

```{r fi.(2}
model.matrix(~ (a + b):c, data.demo) %>% kable
```

--- &vcenter .largecontent

## Formula Interface: `.`

- `.` means all columns of the `data`.

```{r fi..1, echo = TRUE, eval = FALSE}
# ~ Sepal.Length + Sepal.Width + Petal.Length +
#   Petal.Width + Species
model.matrix(~ ., iris.demo)
```

```{r fi..2}
model.matrix(~ ., iris.demo) %>% kable
```


--- &vcenter .largecontent

- Please check `?formula`

--- .dark .segue

## Categorical Features

--- &vcenter .largecontent

## Categorical Feature in R

- A categorical variables of $K$ categories are transformed to a $K-1$-dimentional vector.
- There are many coding systems and the most commonly used is `Dummy Coding`.
    - The first category are transformed to $\vec{0}$.

### Dummy Coding

```{r categorical.variable}
contr.treatment(levels(iris$Species))
```

--- &vcenter .largecontent

## Categorical Feature in Machine Learning

- Predictive analysis
- Regularization
- The categorical variables of $K$ categories are transformed to $K$-dimentional vector.
    - The missing data are transformed to $\vec{0}$.

```{r contrasts.treatment1, echo = TRUE}
contr.treatment(levels(iris.demo$Species), contrasts = FALSE)
```

--- .dark .segue

## Motivation of the FeatureHashing

--- &vcenter .largecontent

## Kaggle: [Display Advertising Challenge](https://www.kaggle.com/c/criteo-display-ad-challenge)

- Given a user and the page he is visiting, what is the probability that he will click on a given ad?

`r center_fig("criteo-logo.png")`

--- &vcenter .largecontent

## The Size of the Dataset

- 13 integer features and 26 categorical features
- $7 \times 10^7$ instances
- Download the dataset via this [link](http://labs.criteo.com/downloads/2014-kaggle-display-advertising-challenge-dataset/)

--- &vcenter .largecontent

## Vectorize These Features in R

- After binning the integer features, I got $3 \times 10^7$ categories.
- Sparse matrix was required
    - A dense matrix required 14PB memory...
    - A sparse matrix required 40GB memory...

--- &vcenter .largecontent

## Estimating Computing Resources

- Dense matrix: `nrow` ($7 \times 10^7$) $\times$ `ncol` ($3 \times 10^7)$ $\times$ 8 bytes
- Sparse matrix: `nrow` ($7 \times 10^7$) $\times$ ($13 + 26$) $\times$ ($4 + 4 + 8$) bytes

--- &vcenter .largecontent

## Vectorize These Features in R

- `sparse.model.matrix` is similar to `model.matrix` but returns a sparse matrix.

--- &vcenter .largecontent

## Fit Model with Limited Memory

- A post [Beat the benchmark with less then 200MB of memory](https://www.kaggle.com/c/criteo-display-ad-challenge/forums/t/10322/beat-the-benchmark-with-less-then-200mb-of-memory) describes how to fit a model with limited resources.
    - online logistic regression
    - feature hash trick
    - adaptive learning rate

--- &vcenter .largecontent

## [Online Machine Learning](https://en.wikipedia.org/wiki/Online_machine_learning)

- In online learning, the model parameter is updated after the arrival of every new datapoint.
    - Only the aggregated information and the incoming datapoint are used.
- In batch learning, the model paramter is updated after access to the entire dataset.

--- &vcenter .largecontent

## Memory Requirement of Online Learning

- Related to model parameters
    - Require little memory for large amount of instances.

--- &vcenter .largecontent

## Limitation of `model.matrix`

- The vectorization requires <font color="red">all categories</font>.

```{r contrasts.need.all.levels, echo = TRUE}
contr.treatment(levels(iris$Species))
```

--- &vcenter .largecontent

## My Work Around

- Scan all data to retrieve all categories
- Vectorize features in an online fashion
- The overhead of exploring features increases

--- &vcenter .largecontent

## Observations

- Mapping features to $\{0, 1, 2, ..., K\}$ is one of method to vectorize feature.
    - `setosa` => $\vec{e_1}$
    - `versicolor` => $\vec{e_2}$
    - `virginica` => $\vec{e_3}$

```{r}
contr.treatment(levels(iris$Species), contrasts = FALSE)
```

--- &vcenter .largecontent

## Observations

- `contr.treatment` ranks all categories to map the feature to integer.
- What if we do not know all categories?
    - Digital features are integer.
    - We use a function maps $\mathbb{Z}$ to $\{0, 1, 2, ..., K\}$

```{r, echo = TRUE}
charToRaw("setosa")
```

--- &vcenter .largecontent

## What is Feature Hashing?

- A method to do feature vectorization with a [hash function](https://en.wikipedia.org/wiki/Hash_function)

`r center_fig("hash.jpeg", 150)`

- For example, Mod `%%` is a family of hash function.

--- &vcenter .largecontent

## Feature Hashing

- Choose a hash function and use it to hash all the categorical features.
- The hash function does not require global information.

--- &vcenter .largecontent

## An Example of Feature Hashing of Criteo's Data

```{r criteo-sample1}
criteo <- fread("dac_sample.txt", data.table = FALSE)
kable(head(criteo[,c(15:17)]))
```

- The categorical variables have been hashed onto 32 bits for anonymization purposes.
- Let us use the last 4 bits as the hash result, i.e. the hash function is `function(x) x %% 16`
- The size of the vector is $(2^4)$, which is called `hash size`

--- &vcenter .largecontent

## An Example of Feature Hashing of Criteo's Data

```{r criteo-sample2}
kable(head(criteo[,c(15:17)]))
```

- `68fd1e64, 80e26c9b, fb936136` => `4, b, 6` 
    - $(0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0)$
- `68fd1e64, f0cf0024, 6f67f7e5` => `4, 4, 5`
    - $(0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)$

--- &vcenter .largecontent

## Hash Collision

- `68fd1e64, f0cf0024, 6f67f7e5` => `4, 4, 5`
    - $(0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)$
- Hash function is a *many to one* mapping, so different features might be mapped to the same index. This is called *collision*.
- In the perspective of statistics, the *collision* in hash function makes the effect of these features confounding.

--- &vcenter .largecontent

## Choosing Hash Function

- Less collision rate
    - Real features, such as text features, are not uniformly distributed in $\mathbb{Z}$.
- High throughput
- `FeatureHashing` uses the [Murmurhash3](https://code.google.com/p/smhasher/wiki/MurmurHash3) algorithm implemented by [`digest`](https://cran.r-project.org/web/packages/digest/index.html)

--- &vcenter .largecontent

## Pros of FeatureHashing

### A Good Companion of Online Algorithm

```{r pros, echo = TRUE, eval = FALSE}
library(FeatureHashing)
hash_size <- 2^16
w <- numeric(hash_size)
for(i in 1:1000) {
  data <- fread(paste0("criteo", i))
  X <- hashed.model.matrix(V1 ~ ., data, hash.size = hash_size)
  y <- data$V1
  update_w(w, X, y)
}
```

--- &vcenter .largecontent

## Pros of FeatureHashing

### A Good Companion of Distributed Algorithm

```{r pros2, echo = TRUE, eval = FALSE}
library(pbdMPI)
library(FeatureHashing)
hash_size <- 2^16
w <- numeric(hash_size)
i <- comm.rank()
data <- fread(paste0("criteo", i))
X <- hashed.model.matrix(V1 ~ ., data, hash.size = hash_size)
y <- data$V1
# ...
```

--- &vcenter .largecontent

## Pros of FeatureHashing

### Simple Training and Testing

```{r pros3, echo = TRUE, eval = FALSE}
library(FeatureHashing)
model <- is_click ~ ad * (url + ip)
m_train <- hashed.model.matrix(model, data_train, hash_size)
m_test <- hashed.model.matrix(model, data_test, hash_size)
```

--- &vcenter .largecontent

## Cons of FeatureHashing

### Hash Size

```{r cons1, cache = TRUE}
criteo.train <- head(criteo, 9000)
y.train <- criteo.train$V1
criteo.test <- tail(criteo, 1000)
y.test <- criteo.test$V1
x <- 10:22
hash_size.set <- 2^x
retval <- numeric(length(hash_size.set))
for(i in seq_along(hash_size.set)) {
  m.train <- hashed.model.matrix(V1 ~ .^2, criteo.train, hash_size.set[i])
  m.test <- hashed.model.matrix(V1 ~ .^2, criteo.test, hash_size.set[i])
  g <- xgboost(m.train, y.train, nrounds = 100, nthread = 8, objective = "binary:logistic", verbose = 0)
  p <- predict(g, m.test)
  retval[i] <- mean(y.test * log(p) + (1 - y.test) * log(1 - p))
}
plot(x, - retval, type = "l", xaxt = "n", xlab = "hash size", ylab = "logarithmic loss")
axis(1, at = x, labels = parse(text = sprintf("2^%d", x)))
```

--- &vcenter .largecontent

## Cons of FeatureHashing

### Lose Interpretation

- Collision makes the interpretation harder.
- It is inconvenient to reverse the indices to feature.

```{r, echo = TRUE}
m <- hashed.model.matrix(~ Species, iris, hash.size = 2^4, create.mapping = TRUE)
hash.mapping(m) %% 2^4
```

--- &vcenter .largecontent

## The Result of the Competition...

- No.1: [Field-aware Factorization Machine](http://ntucsu.csie.ntu.edu.tw/~cjlin/libffm/)
    - [The hashing trick do not contribute any improvement on the leaderboard. They
apply the hashing trick only because it makes their life easier to generate
features.](https://www.kaggle.com/c/criteo-display-ad-challenge/forums/t/10555/3-idiots-solution-libffm/55888#post55888).
- No.3, no.4 and no.9 (we): Neuron Network

--- .dark .segue

## Extending Formula Interface in R

--- &vcenter .largecontent

- **Formula is the most R style interface**

--- &vcenter .largecontent

## Tips

- `terms.formula` and its argument `specials`

```{r tf, dependson="imdb", echo = TRUE}
tf <- terms.formula(~ Plant * Type + conc * split(Treatment), specials = "split", 
                    data = CO2)
attr(tf, "factors")
```

--- &vcenter .largecontent

## Specials

- `attr(tf, "specials")` tells which rows of `attr(tf, "factors")` need to be parsed further

```{r, dependson="tf", echo = TRUE}
rownames(attr(tf, "factors"))
attr(tf, "specials")
```

--- &vcenter .largecontent

## Parse

- `parse` extracts the information from the `specials`

```{r parse, dependson="tf", echo = TRUE}
options(keep.source = TRUE)
p <- parse(text = rownames(attr(tf, "factors"))[4])
getParseData(p)
```

--- &vcenter .largecontent

## Efficiency

- [`Rcpp`](https://cran.r-project.org/web/packages/Rcpp/index.html)
    - The core functions are implemented in C++
- Invoking external C functions
    - `digest` exposes the C function:
        - [Register the c function](https://github.com/eddelbuettel/digest/blob/master/src/init.c)
        - [Add the helper header file](https://github.com/eddelbuettel/digest/blob/master/inst/include/pmurhashAPI.h)
    - `FeatureHashing` imports the C function:
        - [Import the c function](https://github.com/wush978/FeatureHashing/blob/master/src/digestlocal.h)

--- &vcenter .largecontent

## Let's Discuss the Implementation Later

`r center_fig("commpraxis-audience-confusion.png", 75)`
<small>source: http://www.commpraxis.com/wp-content/uploads/2015/01/commpraxis-audience-confusion.png</small>

--- &vcenter .largecontent

## Summary

- Pros of `FeatureHashing`
    - Make it easier to do feature vectorization for predictive analysis.
    - Make it easier to tokenize the text data.
- Cons of `FeatureHashing`
    - Decrease the prediction accuracy if the `hash size` is too small
    - Interpretation becomes harder.

### When should I use `FeatureHashing`?

Short Answer: Predictive Anlysis with a large amount of categories.

--- .dark .segue

## Q&A

--- &vcenter .largecontent

## Thanks You
