---
title       : Introduction of Feature Hashing
subtitle    : Create a Model Matrix via Feature Hashing with a Formula Interface
author      : Wush Wu
job         : Taiwan R User Group
framework   : io2012-wush
highlighter : highlight.js
hitheme     : zenburn
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
knit        : slidify::knit2slides
--- &vcenter .largecontent

```{r setup, include=FALSE, cache=FALSE}
suppressPackageStartupMessages({
  library(knitr)
  library(ggplot2)
  library(xtable)
  library(data.table)
  library(magrittr)
  library(dplyr)
  library(xgboost)
  library(FeatureHashing)
})
opts_chunk$set(echo = FALSE, cache = FALSE)
fig <- function(path, size = 100, style = NULL) {
  if (is.null(style)) sprintf('<img src="assets/img/%s" style="max-height: %d%%; max-width: %d%%;"/>', path, size, size)
  else sprintf('<img src="assets/img/%s" style="%s"/>', path, style)
}
center_fig <- function(path, size = 100, style = NULL) {
  sprintf('<center>%s</center>', fig(path, size, style))
}
```

## Wush, a Co-founder of Taiwan R User Group

- Ph.D. Student of National Taiwan Univ. 
- A Co-founder of Taiwan R User Group
- A Contributor of R Packages

--- &twocol

## Taiwan, a Mountainous Island

*** =left

`r center_fig("TaiwanMap.png", 80)`

*** =right

- Area: 0.5% of Australia's
- Population: 23 M $\approx$ Australia's
- Highest Mountain: Yushan (3952m)

`r center_fig("Mount_Yu_Shan_-_Taiwan.jpg")`

--- &twocol

## Taipei City and Taipei 101

`r center_fig("tpnightview.jpg", 75)`
<small>source: <https://c2.staticflickr.com/6/5208/5231215951_c0e0036b17_b.jpg></small>

---

## Night Market

`r center_fig("nightmarket.jpg", 90)`

source: <http://edition.cnn.com/2014/01/15/travel/10-things-taiwan/>

--- &vcenter .largecontent

## Taiwan R User Group

<http://www.meetup.com/Taiwan-R>

`r center_fig("taiwan-r.jpg", 60)`

--- .segue .dark

## Feature Vectorization and Formula Interface in R

--- &vcenter .largecontent

## Algorithm in Text Book

### Regression

$$y = X \beta + \varepsilon$$

### Real Data

```{r iris}
iris.demo <- iris[c(1:2,51:52,101:102),]
kable(iris.demo)
```

- Where is $X$ and $y$?

--- &vcenter .largecontent

## Feature Vectorization and Formula Interface

- $X$ and $y$ are constructed via `model.matrix` in R

```{r model.matrix, echo = TRUE, results='hide'}
model.matrix(~ ., iris.demo)
```

```{r model.matrix.result}
kable(model.matrix(~ ., iris.demo))
```

--- &vcenter .largecontent

## Formula Interface

- `y ~ model`
    - `y` is modelled by a linear predictor specified symbolically by `model`
- Available symbols is the column names of the `data.frame`. Take `iris` for example:
    - `Sepal.Length`
    - `Sepal.Width`
    - `Petal.Length`
    - `Petal.Width`
    - `Species`

--- &vcenter .largecontent

## Formula Interface

- `model.matrix(~ ., iris)`
    - `.` represents the linear term of all columns but the response

```{r formula, echo = TRUE}
all.equal(
  model.matrix(~., iris),
  model.matrix(~Sepal.Length+Sepal.Width+Petal.Length+Petal.Width+Species, iris)
  )

all.equal(
  model.matrix(Species ~ ., iris),
  model.matrix(Species ~ Sepal.Length+Sepal.Width+Petal.Length+Petal.Width, iris)
  )
```

--- &vcenter .largecontent

## Operator in Formula

- `+`, the basic operator of linear predictors
- `:`, the interaction operator of linear predictor

```{r formula.operator, echo = TRUE, results='hide'}
model.matrix(~ Sepal.Length+Sepal.Width+Sepal.Length:Sepal.Width, iris.demo)
```

```{r formula.operator.result}
kable(model.matrix(~ Sepal.Length+Sepal.Width+Sepal.Length:Sepal.Width, iris.demo))
```

--- &vcenter .largecontent

## Operator in Formula

- `*`, factor crossing. For example, `a*b` is interpreted as `a+b+a:b`
- `^`, the operator indicating crossing to the specified degree

```{r formula.operator.order, echo = TRUE}
all.equal(
  model.matrix(~ Sepal.Length+Sepal.Width+Sepal.Length:Sepal.Width, iris.demo),
  model.matrix(~ Sepal.Length * Sepal.Width, iris.demo)
)

all.equal(
  model.matrix(~ (Sepal.Length + Sepal.Width)^2, iris.demo),
  model.matrix(~ (Sepal.Length + Sepal.Width) * (Sepal.Length + Sepal.Width), iris.demo)
)
```

--- &vcenter .largecontent

- Please check `?formula`

--- .dark .segue

## Categorical Features

--- &vcenter .largecontent

## Categorical Feature in R

- A categorical variable of $K$ categories => a sequence of $K-1$ variables
- There are some built-in contrasts:
    - `contr.treatment`
    - `contr.poly`
    - `contr.sum`
    - `contr.treatment`
    - `contr.SAS`
- <http://www.ats.ucla.edu/stat/r/library/contrast_coding.htm>

--- &vcenter .largecontent

## Contrasts

```{r contrasts.treatment, echo = TRUE}
model.matrix(~ Species, iris.demo)
```

--- &vcenter .largecontent

## Contrasts

```{r contrasts.sum, echo = TRUE}
contrasts(iris.demo$Species) <- contr.sum(levels(iris.demo$Species))
model.matrix(~ Species, iris.demo)
```

--- .dark .segue

## Motivation of the FeatureHashing

--- &vcenter .largecontent

## Modeling Many Categorical Features

- Display Advertising
- Text Mining

`r center_fig("Trouble.jpg")`

<small>source: <http://seckora.com/wp-content/uploads/2014/06/computer-trouble.jpg></small>

--- &vcenter .largecontent

## Display Advertising

`r center_fig("Display_Ads_MatrixAdvertising_Wide.png")`

<small>source: <http://images.dailytech.com/nimage/Display_Ads_MatrixAdvertising_Wide.png></small>

--- &vcenter .largecontent

## Kaggle: [Display Advertising Challenge](https://www.kaggle.com/c/criteo-display-ad-challenge)

- Training Dataset: a portion of [Criteo](http://www.criteo.com/)'s traffic over a period of 7 days.
    - Each instance corresponds to a display ad served by Criteo.
    - The reponse is positive if the ad viewer clicks the ad.
- Evaluation: The [Logarithmic Loss](https://www.kaggle.com/wiki/LogarithmicLoss) of the prediction on the testing dataset.

`r center_fig("criteo-logo.png")`

--- &vcenter .largecontent

## The Size of the Dataset

- 13 integer features and 26 categorical features
- $7 \times 10^7$ instances
- There are many levels of categorical feature which do not appear in the training dataset.
- The [link](http://labs.criteo.com/downloads/2014-kaggle-display-advertising-challenge-dataset/)

--- &vcenter .largecontent

## Vectorize These Features in R

- After binning the integer features, I got $3 \times 10^7$ levels.
- Sparse matrix was required
    - A dense matrix required 14PB memory...
    - A sparse matrix required 40GB memory...

--- &vcenter .largecontent

## Estimating Computing Resources

- Dense matrix: `nrow` ($7 \times 10^7$) $\times$ `ncol` ($3 \times 10^7)$ $\times$ 8 bytes
- Sparse matrix: `nrow` ($7 \times 10^7$) $\times$ ($13 + 26$) $\times$ ($4 + 4 + 8$) bytes

--- &vcenter .largecontent

## Vectorize These Features in R

- There was no implementation of the hashing trick in R, so I used `sparse.model.matrix` with a best machine in [Amazon Web Service](https://aws.amazon.com/tw/).
- It took me many hours to design and implement the algorithm.
- It took me many hours to run the scripts.


--- &vcenter .largecontent

## Fit Model with Limited Memory

- A post [Beat the benchmark with less then 200MB of memory](https://www.kaggle.com/c/criteo-display-ad-challenge/forums/t/10322/beat-the-benchmark-with-less-then-200mb-of-memory) describes how to fit a model with limited resources.
    - online logistic regression
    - feature hash trick
    - adaptive learning rate

--- &vcenter .largecontent

## [Online Machine Learning](https://en.wikipedia.org/wiki/Online_machine_learning)

- In online learning, the model parameter is updated after the arrival of every new datapoint.
- In batch learning, the model paramter is updated after access to the entire dataset.

--- &vcenter .largecontent

## Limitation of `model.matrix`

- **The vectorization requires all levels of categorical features**.

```{r contrasts.need.all.levels, echo = TRUE}
contr.treatment(levels(iris$Species))
```

--- &vcenter .largecontent

## $f : \mathbb{Z} -> {0, 1, 2, ..., k}$

- Mapping features to indices is one of method to vectorize feature. 

```{r contrasts.need.all.levels2, echo = TRUE}
contr.treatment(levels(iris$Species))
```

--- &vcenter .largecontent

## What is Feature Hashing?

- A method to do feature vectorization with a [hash function](https://en.wikipedia.org/wiki/Hash_function)

`r center_fig("hash.jpeg", 150)`

- For example, Mod `%%` is a family of hash function.

--- &vcenter .largecontent

## Feature Hashing

- Choose a hash function and use it to hash all the categorical features.

--- &vcenter .largecontent

## An Example of Feature Hashing of Criteo's Data

```{r criteo-sample1}
criteo <- fread("dac_sample.txt", data.table = FALSE)
kable(head(criteo[,c(15:17)]))
```

- The categorical variables have been hashed onto 32 bits for anonymization purposes.
- Let us use the last 4 bits as the hash result, i.e. the hash function is `function(x) x %% 16`
- The size of the vector is $(2^4)$, which is called `hash size`

--- &vcenter .largecontent

## An Example of Feature Hashing of Criteo's Data

```{r criteo-sample2}
kable(head(criteo[,c(15:17)]))
```

- `68fd1e64, 80e26c9b, fb936136` => `4, b, 6` 
    - $(0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0)$
- `68fd1e64, f0cf0024, 6f67f7e5` => `4, 4, 5`
    - $(0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)$

--- &vcenter .largecontent

## Hash Collision

- `68fd1e64, f0cf0024, 6f67f7e5` => `4, 4, 5`
    - $(0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)$
- Hash function is a *many to one* mapping, so different features might be mapped to the same index. This is called *collision*.
- In the perspective of statistics, the *collision* in hash function makes the effect of these features confounding.

--- &vcenter .largecontent

## Choosing Hash Function

- Less collision rate
    - Real features, such as text features, are not uniformly distributed in $\mathbb{Z}$.
- High throughput
- `FeatureHashing` uses the [Murmurhash3](https://code.google.com/p/smhasher/wiki/MurmurHash3) algorithm implemented by [`digest`](https://cran.r-project.org/web/packages/digest/index.html)

--- &vcenter .largecontent

## Pros of FeatureHashing

### A Good Companion of Online Algorithm

```{r pros, echo = TRUE, eval = FALSE}
library(FeatureHashing)
hash_size <- 2^16
w <- numeric(hash_size)
for(i in 1:1000) {
  data <- fread(paste0("criteo", i))
  X <- hashed.model.matrix(V1 ~ ., data, hash.size = hash_size)
  y <- data$V1
  update_w(w, X, y)
}
```

--- &vcenter .largecontent

## Pros of FeatureHashing

### A Good Companion of Distributed Algorithm

```{r pros2, echo = TRUE, eval = FALSE}
library(pbdMPI)
library(FeatureHashing)
hash_size <- 2^16
w <- numeric(hash_size)
i <- comm.rank()
data <- fread(paste0("criteo", i))
X <- hashed.model.matrix(V1 ~ ., data, hash.size = hash_size)
y <- data$V1
# ...
```

--- &vcenter .largecontent

## Pros of FeatureHashing

### Simple Training and Testing

```{r pros3, echo = TRUE, eval = FALSE}
library(FeatureHashing)
model <- is_click ~ ad * (url + ip)
m_train <- hashed.model.matrix(model, data_train, hash_size)
m_test <- hashed.model.matrix(model, data_test, hash_size)
```

--- &vcenter .largecontent

## Cons of FeatureHashing

### Hash Size

```{r cons1, cache = TRUE}
criteo.train <- head(criteo, 9000)
y.train <- criteo.train$V1
criteo.test <- tail(criteo, 1000)
y.test <- criteo.test$V1
x <- 10:22
hash_size.set <- 2^x
retval <- numeric(length(hash_size.set))
for(i in seq_along(hash_size.set)) {
  m.train <- hashed.model.matrix(V1 ~ .^2, criteo.train, hash_size.set[i])
  m.test <- hashed.model.matrix(V1 ~ .^2, criteo.test, hash_size.set[i])
  g <- xgboost(m.train, y.train, nrounds = 100, nthread = 8, objective = "binary:logistic", verbose = 0)
  p <- predict(g, m.test)
  retval[i] <- mean(y.test * log(p) + (1 - y.test) * log(1 - p))
}
plot(x, - retval, type = "l", xaxt = "n", xlab = "hash size", ylab = "logarithmic loss")
axis(1, at = x, labels = parse(text = sprintf("2^%d", x)))
```

--- &vcenter .largecontent

## Cons of FeatureHashing

### Lose Interpretation

- It is inconvenient to reverse the indices to feature.
- Collision makes the interpretation harder.

```{r, echo = TRUE}
m <- hashed.model.matrix(~ Species, iris, hash.size = 2^4, create.mapping = TRUE)
hash.mapping(m) %% 2^4
```

--- &vcenter .largecontent

## The Result of the Competition...

- No.1: [Field-aware Factorization Machine](http://ntucsu.csie.ntu.edu.tw/~cjlin/libffm/)
- No.3, no.4 and no.9 (we): Neuron Network

--- .dark .segue

## Additional Features of `FeatureHashing`

--- &vcenter .largecontent

## Another Story: Sentiment Analysis

- Provided by [Lewis Crouch](https://github.com/lewis-c)
- Show the Pros of using FeatureHashing

```{r imdb.setup, include=FALSE}
opts_chunk$set(cache = TRUE)
```

--- &vcenter .largecontent

## Dataset: [IMDB](https://www.kaggle.com/c/word2vec-nlp-tutorial)

```{r imdb}
imdb <- read.delim("labeledTrainData.tsv", quote = "", as.is = TRUE)
imdb$review <- tolower(gsub("[^[:alnum:] ]", " ", imdb$review))
```

- Training Dataset: 25000 reviews
- Binary response: positive or negative.

```{r imdb.sentiment, dependson="imdb"}
print(imdb$sentiment[457])
```

- Cleaned review

```{r imdb.review, dependson="imdb"}
strwrap(imdb$review[457], width = 80)
```

```{r imdb.split, dependson="imdb"}
imdb.shuffle <- dplyr::sample_frac(imdb, 1, replace = FALSE)
imdb.train <- head(imdb.shuffle, 20000)
imdb.valid <- tail(imdb.shuffle, 5000)
```

--- &vcenter .largecontent

## Tokenize

```{r imdb.tokenize, dependson="imdb"}
strsplit(imdb$review[457], " ")[[1]]
```


--- &vcenter .largecontent

## Tokenize and Vectorization

```{r imdb.tokenize.vectorize, dependson="imdb"}
tokens <- strsplit(head(imdb$review), " ")
tks <- c("message", "hate", "bad", "like", "gratuitous")
tmp <- lapply(tks, function(tk) {
  sapply(tokens, `%in%`, x = tk)  
}) %>% 
  `names<-`(tks) %>% 
  as.data.frame(stringsAsFactors = FALSE)
tmp$id <- imdb$id %>% head
kable(tmp[,c(6,1:5)])
```

--- &vcenter .largecontent

## Additional Feature of `FeatureHashing`

- `FeatureHashing` implements `split` in the formula interface.

```{r imdb.m, dependson="imdb.split", echo = TRUE}
hash_size <- 2^16
m.train <- hashed.model.matrix(~ split(review, delim = " ", type = "existence"), 
                               data = imdb.train, 
                               hash.size = hash_size,
                               signed.hash = FALSE)
m.valid <- hashed.model.matrix(~ split(review, delim = " ", type = "existence"), 
                              data = imdb.valid, 
                              hash.size = hash_size,
                              signed.hash = FALSE)
```

--- &vcenter .largecontent

## Type of `split`

- `existence`
- `count`
- [`tf-idf`](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) which is contributed by [Michaël Benesty](https://github.com/pommedeterresautee) and will be announced in v0.9.1

--- &vcenter .largecontent

## Gradient Boosted Decision Tree with `xgboost`

```{r, dependson="imdb.m", echo = TRUE, results='hide'}
dtrain <- xgb.DMatrix(m.train, label = imdb.train$sentiment)
dvalid <- xgb.DMatrix(m.valid, label = imdb.valid$sentiment)
watch <- list(train = dtrain, valid = dvalid)
g <- xgb.train(booster = "gblinear", nrounds = 100, eta = 0.0001, max.depth = 2,
                data = dtrain, objective = "binary:logistic",
                watchlist = watch, eval_metric = "auc")
```

```
[0]  train-auc:0.969895	valid-auc:0.914488
[1]	train-auc:0.969982	valid-auc:0.914621
[2]	train-auc:0.970069	valid-auc:0.914766
...
[97]  train-auc:0.975616	valid-auc:0.922895
[98]	train-auc:0.975658	valid-auc:0.922952
[99]	train-auc:0.975700	valid-auc:0.923014
```

--- &vcenter .largecontent

## Performance

- The [AUC](https://www.kaggle.com/wiki/AUC) of `g` is $0.90110$ in the [public leader board](https://www.kaggle.com/c/word2vec-nlp-tutorial/leaderboard?submissionId=1848642) 
    - It outperforms the benchmark in Kaggle

--- &vcenter .largecontent

## The Purpose of FeatureHashing

- **Make our life easier**

--- .dark .segue

## Extending Formula Interface in R

--- &vcenter .largecontent

- **Formula is the most R style interface**

--- &vcenter .largecontent

## Tips

- `terms.formula` and its argument `specials`

```{r tf, dependson="imdb", echo = TRUE}
tf <- terms.formula(~ Plant * Type + conc * split(Treatment), specials = "split", 
                    data = CO2)
attr(tf, "factors")
```

--- &vcenter .largecontent

## Specials

- `attr(tf, "specials")` tells which rows of `attr(tf, "factors")` need to be parsed further

```{r, dependson="tf", echo = TRUE}
rownames(attr(tf, "factors"))
attr(tf, "specials")
```

--- &vcenter .largecontent

## Parse

- `parse` extracts the information from the `specials`

```{r parse, dependson="tf", echo = TRUE}
options(keep.source = TRUE)
p <- parse(text = rownames(attr(tf, "factors"))[4])
getParseData(p)
```

--- &vcenter .largecontent

## Efficiency

- [`Rcpp`](https://cran.r-project.org/web/packages/Rcpp/index.html)
    - The core functions are implemented in C++
- Invoking external C functions
    - `digest` exposes the C function:
        - [Register the c function](https://github.com/eddelbuettel/digest/blob/master/src/init.c)
        - [Add the helper header file](https://github.com/eddelbuettel/digest/blob/master/inst/include/pmurhashAPI.h)
    - `FeatureHashing` imports the C function:
        - [Import the c function](https://github.com/wush978/FeatureHashing/blob/master/src/digestlocal.h)

--- &vcenter .largecontent

## Let's Discuss the Implementation Later

`r center_fig("commpraxis-audience-confusion.png", 75)`
<small>source: http://www.commpraxis.com/wp-content/uploads/2015/01/commpraxis-audience-confusion.png</small>

--- .dark .segue

## Q&A

--- &vcenter .largecontent

## Thanks You

